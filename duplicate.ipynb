{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19125be-4634-4de8-a40f-59c098552eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_recog import Face_recog\n",
    "from face_land_mp import face_landmarks_mp\n",
    "from headpose import head_main\n",
    "from eye_tracker import eye_tracking\n",
    "from imutils import face_utils\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf4e6ab-10ca-494b-ad81-6b7d9db7e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fps(frame):\n",
    "    global pTime\n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime - pTime)\n",
    "    pTime = cTime\n",
    "    cv2.putText(frame, f\"FPS: {int(fps)}\", (60, 70), font, 3, (0,255,0), 2)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635fb304-cf1d-4ffb-b816-4b5b9b4a5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop face based on its bounding box\n",
    "def get_face(frame, bbox):\n",
    "    real_h, real_w, c = frame.shape\n",
    "    x,y,w,h = bbox\n",
    "    y1 = 0 if y < 0 else y\n",
    "    x1 = 0 if x < 0 else x \n",
    "    y2 = real_h if y1 + h > real_h else y + h\n",
    "    x2 = real_w if x1 + w > real_w else x + w\n",
    "    face = frame[y1:y2,x1:x2,:]\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8161a25-4308-4d50-875e-9f119291346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection(frame_in, draw = True, score= False, confidence = 0.85, copy=False):\n",
    "    \"\"\"\n",
    "    Outputs the frame with detected face, alert_bool and cropped face\n",
    "    \"\"\"\n",
    "    global noface_count\n",
    "    global multiple_faces_count\n",
    "    alert_bool = False\n",
    "    bboxes = []\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    if copy:\n",
    "        frame = frame_in.copy()\n",
    "    else:\n",
    "        frame = frame_in\n",
    "    \n",
    "    \n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence= confidence) as face_detector:\n",
    "        \n",
    "        # To improve performance, optionally mark the frame as not writeable to\n",
    "        # pass by reference.\n",
    "        frame.flags.writeable = False\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #Face detection:\n",
    "        results = face_detector.process(frame)\n",
    "        frame.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "    # Absence of any face detection\n",
    "    if not results.detections:\n",
    "        noface_count+=1\n",
    "        alert_bool = True\n",
    "        cv2.putText(frame, 'Alert! No faces detected for '+str(noface_count)+' times', (30, 30), font, 1, (0, 255, 255), 2)\n",
    "    # Multiple faces detection\n",
    "    elif len(results.detections)>1:\n",
    "        multiple_faces_count += 1\n",
    "        alert_bool = True\n",
    "        cv2.putText(frame, 'Alert! multiple faces detected for '+str(multiple_faces_count)+' times', (30, 30), font, 1, (0, 255, 255), 2)\n",
    "    \n",
    "    # Get bboxes for detected faces and draw the face detection annotations on the frame.\n",
    "    if results.detections:\n",
    "        for id, detection in enumerate(results.detections):\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            ih, iw, ic = frame.shape\n",
    "            bbox = int(bbox.xmin * iw), int(bbox.ymin * ih), int(bbox.width * iw), int(bbox.height * ih)\n",
    "            bboxes.append(bbox)\n",
    "            if draw:\n",
    "                cv2.rectangle(frame, bbox, (255, 0, 255), 2)\n",
    "                if score:\n",
    "                    cv2.putText(frame, f'{int(detection.score[0] * 100)}%', (bbox[0], bbox[1] - 20), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 255), 2)\n",
    "    \n",
    "    return alert_bool, frame, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53784fc0-8427-4b53-82b2-314268a17951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_recognition(frame, fr, bbox):\n",
    "    global failed_verif_count\n",
    "    \n",
    "    # Detect and recognise Faces\n",
    "    face_locations, face_names = fr.detect_known_faces(frame)\n",
    "    \n",
    "    alert_bool = not face_names\n",
    "    #Draw box and name\n",
    "    for face_loc, name in zip(face_locations, face_names):\n",
    "        cv2.putText(frame, name, (bbox[0], bbox[1] - 20), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 255), 2)\n",
    "        \n",
    "    if not alert_bool:\n",
    "        cv2.putText(frame, 'You are verified', (30, 30), font, 1, (0, 255, 255), 2)\n",
    "    else:\n",
    "        failed_verif_count += 1\n",
    "        cv2.putText(frame, 'Alert! You are not the actual user: '+str(failed_verif_count), (30, 30), font, 1, (0, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26f23d3-17dc-45ab-87b2-21911502d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facial_landmarks_detection(frame_in, draw= True, copy=False, module = \"Dlib\"):\n",
    "    if copy:\n",
    "        frame = frame_in.copy()\n",
    "    else:\n",
    "        frame = frame_in\n",
    "    \n",
    "    if module == \"Dlib\":\n",
    "        # detect faces in the grayscale image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)   \n",
    "        rects = detector(gray, 0)\n",
    "        ret = False\n",
    "        shape = \"\"\n",
    "    \n",
    "        # loop over the face detections\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            # determine the facial landmarks for the face region, then\n",
    "            # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "            # array\n",
    "            ret = True\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "        \n",
    "            # loop over the (x, y)-coordinates for the facial landmarks\n",
    "            # and draw them on the frame\n",
    "            if draw:\n",
    "                for (x, y) in shape:\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "                    \n",
    "        return ret, shape, frame\n",
    "    \n",
    "    elif module == \"mediapipe\":\n",
    "        alert_bool, shape, frame = face_landmarks_mp(frame)\n",
    "        return False, shape, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5138bf28-df19-4591-9f0a-bb9d9bdefaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_image():\n",
    "    while True:\n",
    "        blank = cv2.imread('db/blank.png')\n",
    "        cv2.putText(blank, 'press r to capture image', (30, 30), font, 1, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Output\", blank)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('r'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # capturing image\n",
    "    webcam = cv2.VideoCapture(0) \n",
    "    ret, frame = webcam.read()\n",
    "    \n",
    "    # saving image as use_image.jpg\n",
    "    # for further face verification\n",
    "    cv2.imwrite(\"captures/user_image.jpg\", frame)\n",
    "    webcam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # reading picture as user_pic\n",
    "    user_pic = cv2.imread('captures/user_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47386dbb-e78e-486e-bf7a-87f97cec1d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 encoding images found.\n",
      "Encoding images loaded\n"
     ]
    }
   ],
   "source": [
    "font = cv2.FONT_HERSHEY_PLAIN \n",
    "\n",
    "# capture user image\n",
    "capture_image()\n",
    "\n",
    "# Face recognizer\n",
    "fr = Face_recog()\n",
    "fr.load_encoding_images(\"captures/\")\n",
    "\n",
    "# Facial landmarks predictor\n",
    "saved_model = \"models/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(saved_model)\n",
    "\n",
    "noface_count = 0\n",
    "multiple_faces_count = 0\n",
    "failed_verif_count = 0\n",
    "pTime = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0c9c75-3f07-4e8a-86f2-dfcf57003453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cv2.namedWindow('PROCTORING ON')\n",
    "    while(True):\n",
    "        # Video capture frame by frame\n",
    "        ret, frame = cap.read()\n",
    "        frame = print_fps(cv2.flip(frame, 1))\n",
    "        \n",
    "        #Faces detection\n",
    "        alert_bool, frame, bboxes =  face_detection(frame, confidence = 0.95, copy=False)\n",
    "        \n",
    "        #Only if single face detected\n",
    "        if not alert_bool:    \n",
    "            \n",
    "            #Face verification\n",
    "            face_recognition(frame, fr, bboxes[0])\n",
    "            \n",
    "            # Facial landmarks detection\n",
    "            ret, shape, frame = facial_landmarks_detection(frame, copy=False, module=\"Dlib\", draw= True)\n",
    "            if ret:\n",
    "                head_main(frame,shape, copy=False)\n",
    "                eye_tracking(frame, shape, threshold = 75)\n",
    "        \n",
    "        cv2.imshow('PROCTORING ON',  frame)\n",
    "                \n",
    "        if cv2.waitKey(1) & 0xFF == 27: \n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58be2877-3649-451a-8414-56e1648ce587",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anirudh\\mini_project_iiita\\headpose.py:134: RuntimeWarning: divide by zero encountered in int_scalars\n",
      "  m = (x2[1] - x1[1])/(x2[0] - x1[0])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "144bf861-1877-466c-914f-be8e41d726c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    DO NOT DELETE THIS!\n",
    "# Warnings in head pose estimation:\n",
    "#     C:\\Users\\Anirudh\\mini_project_iiita\\headpose.py:129: RuntimeWarning: divide by zero encountered in int_scalars\n",
    "#   m = (x2[1] - x1[1])/(x2[0] - x1[0])\n",
    "#   ang2 = ...(1/m)..\n",
    "# C:\\Users\\Anirudh\\mini_project_iiita\\eye_tracker.py:39: RuntimeWarning: divide by zero encountered in long_scalars\n",
    "#   y_ratio = (cy - end_points[1])/(end_points[3] - cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bbe44ce-3262-4d2c-9182-900acfb22162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough:\n",
    "        # outputs: detreg_out, landeye_out, head_out\n",
    "#         head_out = cv2.copyMakeBorder(head_out, 0, 0, 320, 320, cv2.BORDER_CONSTANT, (0,0,0))\n",
    "#         horiz = np.concatenate((horiz, head_out), axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
